"""
Deduplica o m√™s CORRENTE sem consolidar em 1 arquivo.
Mant√©m m√∫ltiplos arquivos, mas garante uniqueness.
Roda DIARIAMENTE ap√≥s o pipeline principal.
"""

import boto3
import pandas as pd
import logging
from datetime import datetime
import os
import hashlib

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

BUCKET = "20-ze-datalake-landing"
BASE_PREFIX = "osrm_distance/osrm_landing"

def dedupe_current_month():
    """L√™ TODOS os arquivos do m√™s corrente, remove duplicatas, reescreve."""
    
    current_month = datetime.now().strftime('%Y-%m')
    year, month = current_month.split('-')
    prefix = f"{BASE_PREFIX}/year={year}/month={month}/"
    
    logging.info("="*60)
    logging.info(f"üìÖ Deduplicando m√™s corrente: {current_month}")
    logging.info("="*60)
    
    s3 = boto3.client('s3')
    
    # 1. Lista arquivos
    paginator = s3.get_paginator('list_objects_v2')
    files = []
    
    for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
        if 'Contents' in page:
            for obj in page['Contents']:
                key = obj['Key']
                # Ignora arquivos j√° consolidados
                if key.endswith('.parquet') and 'consolidated' not in key:
                    files.append(key)
    
    if not files:
        logging.warning(f"‚ö†Ô∏è  Nenhum arquivo encontrado")
        return
    
    logging.info(f"üìÇ Encontrados {len(files)} arquivo(s)")
    
    # 2. L√™ TODOS os arquivos
    dfs = []
    for idx, file_key in enumerate(files):
        local_file = f"temp_{idx}.parquet"
        
        try:
            s3.download_file(BUCKET, file_key, local_file)
            df = pd.read_parquet(local_file)
            dfs.append(df)
            os.remove(local_file)
            
            logging.info(f"‚úÖ [{idx+1}/{len(files)}] Lido: {os.path.basename(file_key)} ({len(df):,} registros)")
            
        except Exception as e:
            logging.error(f"‚ùå Erro: {e}")
            if os.path.exists(local_file):
                os.remove(local_file)
            continue
    
    # 3. Concatena e deduplica
    df_full = pd.concat(dfs, ignore_index=True)
    total_before = len(df_full)
    
    logging.info(f"üìä Total ANTES: {total_before:,}")
    
    df_dedupe = df_full.drop_duplicates(subset=['order_number'], keep='first')
    total_after = len(df_dedupe)
    
    logging.info(f"üìä Total AP√ìS: {total_after:,}")
    logging.info(f"üóëÔ∏è  Removidas: {total_before - total_after:,} duplicatas")
    
    # 4. Divide em chunks para manter compatibilidade com DAG
    chunk_size = 1_000_000
    num_chunks = (len(df_dedupe) // chunk_size) + 1
    
    execution_hash = hashlib.md5(datetime.now().isoformat().encode()).hexdigest()[:8]
    
    logging.info(f"üíæ Salvando {num_chunks} arquivo(s)...")
    
    new_files = []
    for i in range(num_chunks):
        start = i * chunk_size
        end = start + chunk_size
        chunk = df_dedupe[start:end]
        
        if len(chunk) == 0:
            continue
        
        # Nome com hash √∫nico + timestamp
        filename = f"dedupe_{execution_hash}_{i:03d}.parquet"
        local_path = filename
        s3_key = f"{prefix}{filename}"
        
        chunk.to_parquet(local_path, index=False)
        s3.upload_file(local_path, BUCKET, s3_key)
        os.remove(local_path)
        
        new_files.append(s3_key)
        logging.info(f"   ‚úÖ Salvo: {filename} ({len(chunk):,} registros)")
    
    # 5. DELETE arquivos antigos
    logging.warning(f"üóëÔ∏è  Deletando {len(files)} arquivo(s) antigo(s)...")
    
    for file_key in files:
        try:
            s3.delete_object(Bucket=BUCKET, Key=file_key)
        except Exception as e:
            logging.error(f"Erro ao deletar {file_key}: {e}")
    
    logging.info(f"‚úÖ Dedupe conclu√≠do!")
    logging.info(f"   Arquivos: {len(files)} ‚Üí {len(new_files)}")
    logging.info(f"   Registros: {total_before:,} ‚Üí {total_after:,}")

if __name__ == "__main__":
    dedupe_current_month()